{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e0a9426",
   "metadata": {},
   "source": [
    "# RAG System\n",
    "Imagine you're asking a super smart friend (that's the Large Language Model or LLM) a question. A RAG system is like giving your super smart friend a quick way to look things up before answering.\n",
    "How RAG works:\n",
    "\n",
    "1.  **Documents are Chunked and Embedded:** Your knowledge base is broken into small pieces, and each piece is converted into a numerical \"meaning\" representation.\n",
    "2.  **Embeddings are Stored:** These numerical representations are then saved in a special database designed for quick similarity searches.\n",
    "3.  **User Submits a Query:** You ask your question to the RAG system.\n",
    "4.  **Query is Embedded:** Your question is also converted into a numerical \"meaning\" representation.\n",
    "5.  **Relevant Chunks are Retrieved:** The system searches its database for document chunks whose meanings are most similar to your query's meaning.\n",
    "6.  **Context is Formed:** The retrieved relevant text chunks are then added to your original query, creating an enriched prompt.\n",
    "7.  **LLM Generates Answer:** A Large Language Model uses this enriched prompt to provide a factual and comprehensive response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9b4b85-9d17-468c-b244-73a1c5191e3e",
   "metadata": {},
   "source": [
    "### RAG application built on gemini "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409200ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.13.2' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# install required packages\n",
    "! pip install langchain langchain-community langchain-google-genai python-dotenv streamlit langchain-experimental sentence-transformers langchain-chroma langchainhub pypdf rapidocr-onnxruntime\n",
    "! pip install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d49c22a-1ad0-4395-b93b-aa95660aa026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"mypdf.pdf\")\n",
    "data = loader.load()  \n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a374eb7c-e262-42bb-8f3f-308ba7dcdbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29633e3b-ff24-4ace-a09b-c03b6e28c5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# split data\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "\n",
    "print(\"Total number of documents: \",len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100b7d1a-1209-49d4-99ed-c51bc233a938",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7016c1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713180e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed96936",
   "metadata": {},
   "source": [
    "### Get an API key: \n",
    "Head to https://ai.google.dev/gemini-api/docs/api-key to generate a Google AI API key. Paste in .env file\n",
    "\n",
    "Embedding models: https://python.langchain.com/v0.1/docs/integrations/text_embedding/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8981bf9d",
   "metadata": {},
   "source": [
    "**A vector database** stores data as numerical \"vectors\" to enable semantic search, finding items based on meaning rather than keywords.   \n",
    "**ChromaDB** is an easy-to-use, open-source vector database that simplifies storing and querying these embeddings, with built-in persistence for your data.   \n",
    "**FAISS** is a high-performance library for efficient similarity search and clustering of dense vectors, widely used for building scalable vector search applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8b2a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1073ab7f-2632-4367-8dec-c19449d6ce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vector = embeddings.embed_query(\"hello, world!\")\n",
    "vector\n",
    "#vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688b6e6a-d8ab-41fb-a665-b72c9c9b4026",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(documents=docs, embedding=embeddings)\n",
    "# vectorstore = Chroma.from_documents(documents=docs, embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30c2008",
   "metadata": {},
   "source": [
    "The `retriever` enables efficient semantic search by retrieving the most relevant document chunks from the vectorstore based on the user's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c674c5c-1b57-42e9-a99d-9e882c75da2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"Location of Himalayan Engineering College?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c5c6bb-fd0e-45ec-b315-e3f7656e0329",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce764bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1c8321-1efd-4a11-9744-0d1a7c6f4e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retrieved_docs[5].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2fae69",
   "metadata": {},
   "source": [
    "### Langchain \n",
    "LangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs). It provides tools for connecting LLMs to external data sources, enabling retrieval-augmented generation, and building advanced conversational and reasoning workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f991a1f-6ce9-4463-9941-b35014df94f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",temperature=0.3, max_tokens=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba4bb1a",
   "metadata": {},
   "source": [
    "**Prompt engineering** is the practice of crafting effective inputs (prompts) to guide large language models (LLMs) toward producing accurate, relevant, and useful outputs.  \n",
    "It involves experimenting with wording, structure, and context to optimize model responses for specific tasks or applications.  \n",
    "Good prompt engineering can significantly improve the quality and reliability of LLM-powered solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee17439-7bc3-4931-9f57-4ec7e82ce902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266e86e0-746b-4943-9470-fd842633ed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db9500d-4c51-4a10-9b21-f1ef9c8f985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke({\"input\": \"who is the principle of Himalayan Engineering College?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff65d0-2436-47f8-8572-6979a3378701",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke({\"input\": \"WELL TELL  ME ABOUT THE COURSE OF COMPUTER ENGINEERIING\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b8f1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke({\"input\": \"WHAT IS HCOE POPULAR FOR\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
